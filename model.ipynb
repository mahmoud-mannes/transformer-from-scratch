{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8SwMvOCkaaqh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import math\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameters (setting the embedding_dim to be a multiple of d_key is ideal)\n",
        "embedding_dim = 384\n",
        "max_token = 128\n",
        "d_key = 64\n",
        "n_decoder_layers = 6\n",
        "n_head = embedding_dim // d_key\n",
        "mini_batch_size = 64\n",
        "num_epochs = 4000\n",
        "decay = 0.01\n",
        "learning_rate = 3e-4\n",
        "PATIENCE = 10\n",
        "drop_prob = 0.5\n",
        "beta1 = 0.9\n",
        "beta2 = 0.999\n",
        "label_smoothing = 0.05\n",
        "desired_vocab = 1000 #Final vocab after tokenization, set to 65 if you wish for no tokenization."
      ],
      "metadata": {
        "id": "x1objpJPa47u"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BPE based tokenizer\n",
        "def tokenize(data, initial_vocab, desired_vocab=1000, min_freq=2):\n",
        "\n",
        "    # Map ids back to strings\n",
        "    vocab = list(initial_vocab)\n",
        "    itos = {i: vocab[i] for i in range(len(vocab))}\n",
        "    seq = [itos[i] for i in data]  # work with strings\n",
        "\n",
        "    while len(vocab) < desired_vocab:\n",
        "        # count adjacent pairs\n",
        "        pairs = Counter(zip(seq, seq[1:]))\n",
        "        if not pairs:\n",
        "            break\n",
        "\n",
        "        # Get most common pair\n",
        "        (a, b), freq = pairs.most_common(1)[0]\n",
        "        if freq < min_freq:\n",
        "            # Stop if even the best merge is too rare\n",
        "            break\n",
        "\n",
        "        new_token = a + b\n",
        "        if new_token in vocab:\n",
        "            # already in vocab, skip\n",
        "            break\n",
        "\n",
        "        # Merge occurrences in sequence\n",
        "        new_seq = []\n",
        "        i = 0\n",
        "        while i < len(seq):\n",
        "            if i < len(seq)-1 and seq[i] == a and seq[i+1] == b:\n",
        "                new_seq.append(new_token)\n",
        "                i += 2\n",
        "            else:\n",
        "                new_seq.append(seq[i])\n",
        "                i += 1\n",
        "        seq = new_seq\n",
        "        vocab.append(new_token)\n",
        "\n",
        "        if len(vocab) % 100 == 0:\n",
        "            print(f\"Current vocab size: {len(vocab)}\")\n",
        "\n",
        "    # Rebuild final mappings\n",
        "    itos = {i: tok for i, tok in enumerate(vocab)}\n",
        "    stoi = {tok: i for i, tok in itos.items()}\n",
        "\n",
        "    data = [stoi[tok] for tok in seq]\n",
        "    return data, vocab\n",
        "#Train Dev Split (Random shuffling will be done later)\n",
        "def train_dev_split(x,train,dev):\n",
        "    N = x.shape[0]\n",
        "    Ntr = int(N * train)\n",
        "    Ndev = int(N*dev)\n",
        "\n",
        "    data_tr = x[:Ntr]\n",
        "    data_dev = x[Ntr:Ntr+Ndev]\n",
        "    return data_tr,data_dev\n",
        "\n",
        "#Final data preparation function\n",
        "def prepare_data(path, max_token=8):\n",
        "    #Read data, setup vocab (pre-tokenization), encoder and decoder functions\n",
        "    data = open(path).read()\n",
        "    vocab = sorted(list(set(data)))\n",
        "    vocab_size = len(vocab)\n",
        "\n",
        "    itos = {i:vocab[i] for i in range(vocab_size)}\n",
        "    stoi = {v:k for k,v in itos.items()}\n",
        "\n",
        "    encode = lambda inp: [stoi[i] for i in inp]\n",
        "    decode = lambda inp: \"\".join([itos[i] for i in inp])\n",
        "    #Tokenize, redo the vocab, encoder and decoder (post-tokenization)\n",
        "    print(\"Tokenization start..\")\n",
        "    data,vocab = tokenize(encode(data), vocab)\n",
        "    data = torch.tensor(data)\n",
        "    print(\"Tokenization end.\")\n",
        "    vocab_size = len(vocab)\n",
        "\n",
        "    itos = {i:vocab[i] for i in range(vocab_size)}\n",
        "    stoi = {v:k for k,v in itos.items()}\n",
        "\n",
        "    encode = lambda inp: [stoi[i] for i in inp]\n",
        "    decode = lambda inp: \"\".join([itos[i] for i in inp])\n",
        "    #Train dev split, arrange data into B,T,C shape, random shuffling of data.\n",
        "    Batch_data = data.shape[0] // (max_token+1)\n",
        "\n",
        "    ranges = torch.arange(Batch_data).view(Batch_data,1) + torch.arange((max_token + 1))\n",
        "    data = data[ranges][torch.randperm(Batch_data)]\n",
        "\n",
        "    data_tr,data_dev = train_dev_split(data,0.8,0.2)\n",
        "    return {\"data\": data, \"data_tr\": data_tr, \"data_dev\": data_dev, \"vocab\": vocab, \"encode\": encode, \"decode\": decode}"
      ],
      "metadata": {
        "id": "1-Y22J3faj6h"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Simple function to create random batch from data\n",
        "def create_batch(data, mini_batch_size = mini_batch_size):\n",
        "    ind = torch.randperm(data.shape[0])\n",
        "    shuffled_data = data[ind]\n",
        "    ind = None\n",
        "    return shuffled_data[:mini_batch_size]"
      ],
      "metadata": {
        "id": "aeYLBCK7bRX0"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data prep + Tokenization\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "full_data = prepare_data(\"input.txt\", max_token)\n",
        "data, data_tr, data_dev, vocab, encode, decode = full_data[\"data\"].tolist(),full_data[\"data_tr\"],full_data[\"data_dev\"], full_data[\"vocab\"], full_data[\"encode\"], full_data[\"decode\"]\n",
        "data_tr = data_tr.to(device)\n",
        "data_dev = data_dev.to(device)\n",
        "vocab_size = len(vocab)"
      ],
      "metadata": {
        "id": "J5W4pjzeb9BY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.keyM = nn.Linear(embedding_dim, d_key)\n",
        "        self.queryM = nn.Linear(embedding_dim, d_key)\n",
        "        self.valueM = nn.Linear(embedding_dim, d_key)\n",
        "        self.dropout = nn.Dropout(p=drop_prob)\n",
        "    def forward(self,x):\n",
        "        Q = self.queryM(x)\n",
        "        K = self.keyM(x)\n",
        "        V = self.valueM(x)\n",
        "        scores = (Q @ K.permute(0,2,1)) / torch.sqrt(torch.tensor(d_key,device=device))\n",
        "        #Masking for attention\n",
        "        inp = torch.ones(x.shape[1], x.shape[1], device=device)\n",
        "        mask = torch.tril(inp).bool()\n",
        "        scores = scores.masked_fill(~mask, float('-inf'))\n",
        "        #Attention\n",
        "        attn = self.dropout(torch.softmax(scores, dim=-1))\n",
        "        return attn @ V\n",
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head() for i in range(n_head)])\n",
        "        self.linear_layer = nn.Linear(n_head*d_key, embedding_dim)\n",
        "    def forward(self,x):\n",
        "        res = []\n",
        "        for i in range(n_head):\n",
        "            res.append(self.heads[i](x))\n",
        "        return self.linear_layer(torch.cat(res, dim=-1))\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embedding_table = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.pos_table = nn.Embedding(max_token,embedding_dim)\n",
        "        self.dropout = nn.Dropout(p=drop_prob)\n",
        "        self.decoder_layers = nn.ModuleList([MultiHeadedAttention() for i in range(n_decoder_layers)])\n",
        "        self.feed_forward = nn.ModuleList([nn.Sequential(nn.Linear(embedding_dim,2*embedding_dim),nn.ReLU(),nn.Dropout(p=drop_prob),nn.Linear(2*embedding_dim,embedding_dim)) for i in range(n_decoder_layers)])\n",
        "        self.final_linear = nn.Linear(embedding_dim,vocab_size, bias = False)\n",
        "        self.layer_norm1 = nn.LayerNorm(embedding_dim)\n",
        "        self.layer_norm2 = nn.LayerNorm(embedding_dim)\n",
        "    def forward(self,x):\n",
        "        emb = self.embedding_table(x) + self.pos_table(torch.arange(x.shape[1],device=device))\n",
        "        out = self.dropout(emb)\n",
        "        for i in range(n_decoder_layers):\n",
        "            attn_out = self.decoder_layers[i](self.layer_norm1(out))\n",
        "            out = out + self.dropout(attn_out)\n",
        "            ff_out = self.feed_forward[i](self.layer_norm2(out))\n",
        "            out = out + self.dropout(ff_out)\n",
        "        out = self.dropout(out)\n",
        "        return self.final_linear(out)\n",
        "    def generate(self):\n",
        "      T = 0.7\n",
        "      K = 100\n",
        "      out = ''\n",
        "      current_context = [i.item() for i in torch.randint(low=0,high=vocab_size - 1,size=(1,))]\n",
        "      while True:\n",
        "        if len(current_context) > max_token:\n",
        "          current_context = current_context[1:]\n",
        "        logits = DecoderOnlyTransformer(torch.tensor([current_context],device=device))[0,-1] / T\n",
        "        topk_logits, topk_indices = torch.topk(logits, K)\n",
        "        probs = torch.softmax(topk_logits, dim = -1)\n",
        "        #topK = sorted(probs)[-K:]\n",
        "        pred = topk_indices[torch.multinomial(probs,num_samples = 1).item()].item()\n",
        "        out = out + decode([pred])\n",
        "        current_context.append(pred)\n",
        "        print(decode([pred]), end=\"\")"
      ],
      "metadata": {
        "id": "4BbNV1s-caX3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining a function for warmup and cosine decay for more stable training near the end of the training phase.\n",
        "warmup_steps = 2000\n",
        "def lambda_lr(current_step):\n",
        "  if current_step <= warmup_steps:\n",
        "    return float(current_step) / float(max(1,warmup_steps))\n",
        "  progress = float(current_step - warmup_steps) / float(max(1, num_epochs - warmup_steps))\n",
        "  return 0.5 * (1.0 + math.cos(math.pi * progress))"
      ],
      "metadata": {
        "id": "jJSKo0Lg7Dqu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialization of Transformer, optimizer and optimization scheduler.\n",
        "DecoderOnlyTransformer = Transformer().to(device)\n",
        "optimizer = torch.optim.AdamW(lr=learning_rate, betas =(beta1, beta2), eps=1e-8, weight_decay = decay,params=DecoderOnlyTransformer.parameters())\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda_lr)"
      ],
      "metadata": {
        "id": "YkyW2Es5cpp8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TRAINING BLOCK:\n",
        "for i in range(num_epochs):\n",
        "    DecoderOnlyTransformer.train()\n",
        "\n",
        "    minibatch = create_batch(data_tr)\n",
        "    Xbatch,Ybatch = minibatch[:,:max_token], minibatch[:,1:max_token+1]\n",
        "\n",
        "    out = DecoderOnlyTransformer(Xbatch)\n",
        "    B,T,C = out.shape\n",
        "    out = out.view(B*T, C)\n",
        "    Ybatch = Ybatch.reshape(-1)\n",
        "\n",
        "    loss = F.cross_entropy(out,Ybatch,label_smoothing=label_smoothing)\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if i % 100 == 0:\n",
        "      print(f'Training loss: {loss}')\n",
        "\n",
        "    DecoderOnlyTransformer.eval()\n",
        "    with torch.inference_mode():\n",
        "        dev_batch = create_batch(data_dev, 500)\n",
        "\n",
        "        out = DecoderOnlyTransformer(dev_batch[:,:max_token])\n",
        "        targets = dev_batch[:,1:max_token+1]\n",
        "\n",
        "        B,T,C = out.shape\n",
        "        out = out.view(B*T, C)\n",
        "        targets = targets.reshape(-1)\n",
        "\n",
        "        dev_loss = F.cross_entropy(out,targets,label_smoothing=label_smoothing)\n",
        "        if i % 100 == 0:\n",
        "          print(f'Dev loss: {dev_loss}')\n",
        "\n",
        "        if i == 0:\n",
        "\n",
        "          best = dev_loss\n",
        "          patience = 0\n",
        "\n",
        "        elif dev_loss < best:\n",
        "\n",
        "          best = dev_loss\n",
        "          torch.save(DecoderOnlyTransformer.state_dict(),\"best.pt\")\n",
        "          patience = 0\n",
        "\n",
        "        else:\n",
        "\n",
        "          patience += 1\n",
        "          if patience > PATIENCE and dev_loss - best >= 0.3:\n",
        "            print(\"Early stopping.\")\n",
        "            break"
      ],
      "metadata": {
        "id": "4uCQQQd5cvqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DecoderOnlyTransformer.generate()"
      ],
      "metadata": {
        "id": "OqQmtHpoj6GO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}